{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLNLP W4 Notebook 3\n",
    "## Fully connected model for SS prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/Users/frellsen/Dropbox/Share/Permanent/DLNLP2018/cullpdb+profile_6133_ss3-preprocessed.npz')\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_validation = data['X_validation']\n",
    "y_validation = data['y_validation']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the model/graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3362900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Input and output\n",
    "X = tf.placeholder(tf.float32, [None, 700, 44], name=\"X\")\n",
    "y = tf.placeholder(tf.float32, [None, 700, 4], name='y')\n",
    "\n",
    "# Defined the model parameters\n",
    "hidden = 100\n",
    "W1 = tf.get_variable(\"W1\", [700 * 44, hidden], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.get_variable(\"b1\", [hidden], initializer=tf.random_normal_initializer())\n",
    "W2 = tf.get_variable(\"W2\", [hidden, 700 * 4], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.get_variable(\"b2\", [700 * 4], initializer=tf.random_normal_initializer())\n",
    "\n",
    "# Construct model\n",
    "z1 = tf.nn.relu(tf.matmul(tf.layers.flatten(X), W1) + b1)\n",
    "a2 = tf.reshape(tf.matmul(z1, W2) + b2, [-1, 700, 4])\n",
    "y_ = tf.nn.softmax(a2)\n",
    "\n",
    "# Then we mask out the NoSeq\n",
    "mask = tf.not_equal(tf.argmax(y, 2), 3)\n",
    "\n",
    "y_masked = tf.boolean_mask(y, mask)\n",
    "a2_masked = tf.boolean_mask(a2, mask)\n",
    "y__masked = tf.boolean_mask(y_, mask)\n",
    "\n",
    "# Difine the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_masked, logits=a2_masked))\n",
    "\n",
    "# Define the optimizer operation\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "# Variables for prediction and accuracy\n",
    "prediction = tf.argmax(y__masked, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(y_masked, 1)), tf.float32))\n",
    "\n",
    "# Initialize the variables (they are assigned default values)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_parameters = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "print(\"Number of parameters:\", n_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "loss[b=0000] = 1.802155, val_acc = 0.242607\n",
      "loss[b=1000] = 1.712274, val_acc = 0.257421\n",
      "loss[b=2000] = 1.583822, val_acc = 0.293727\n",
      "loss[b=3000] = 1.439679, val_acc = 0.331930\n",
      "loss[b=4000] = 1.289641, val_acc = 0.366794\n",
      "loss[b=5000] = 1.178357, val_acc = 0.387773\n",
      "Epoch: 1\n",
      "loss[b=0000] = 1.142702, val_acc = 0.390884\n",
      "loss[b=1000] = 1.122578, val_acc = 0.392686\n",
      "loss[b=2000] = 1.096569, val_acc = 0.395816\n",
      "loss[b=3000] = 1.094561, val_acc = 0.397390\n",
      "loss[b=4000] = 1.068161, val_acc = 0.400027\n",
      "loss[b=5000] = 1.061720, val_acc = 0.402170\n",
      "Epoch: 2\n",
      "loss[b=0000] = 1.061193, val_acc = 0.401677\n",
      "loss[b=1000] = 1.075919, val_acc = 0.400804\n",
      "loss[b=2000] = 1.068749, val_acc = 0.404427\n",
      "loss[b=3000] = 1.074577, val_acc = 0.404560\n",
      "loss[b=4000] = 1.055802, val_acc = 0.406191\n",
      "loss[b=5000] = 1.051662, val_acc = 0.407747\n",
      "Epoch: 3\n",
      "loss[b=0000] = 1.052595, val_acc = 0.407728\n",
      "loss[b=1000] = 1.068608, val_acc = 0.405660\n",
      "loss[b=2000] = 1.062011, val_acc = 0.407880\n",
      "loss[b=3000] = 1.067754, val_acc = 0.408790\n",
      "loss[b=4000] = 1.049202, val_acc = 0.411313\n",
      "loss[b=5000] = 1.045655, val_acc = 0.411958\n",
      "Epoch: 4\n",
      "loss[b=0000] = 1.046529, val_acc = 0.413893\n",
      "loss[b=1000] = 1.063193, val_acc = 0.411218\n",
      "loss[b=2000] = 1.055985, val_acc = 0.412622\n",
      "loss[b=3000] = 1.061713, val_acc = 0.414348\n",
      "loss[b=4000] = 1.042636, val_acc = 0.416586\n",
      "loss[b=5000] = 1.039677, val_acc = 0.419280\n",
      "Epoch: 5\n",
      "loss[b=0000] = 1.039853, val_acc = 0.418976\n",
      "loss[b=1000] = 1.057106, val_acc = 0.418198\n",
      "loss[b=2000] = 1.048481, val_acc = 0.419773\n",
      "loss[b=3000] = 1.054555, val_acc = 0.422751\n",
      "loss[b=4000] = 1.034948, val_acc = 0.424401\n",
      "loss[b=5000] = 1.032747, val_acc = 0.426203\n",
      "Epoch: 6\n",
      "loss[b=0000] = 1.031277, val_acc = 0.427095\n",
      "loss[b=1000] = 1.049192, val_acc = 0.427341\n",
      "loss[b=2000] = 1.038732, val_acc = 0.427872\n",
      "loss[b=3000] = 1.045851, val_acc = 0.434075\n",
      "loss[b=4000] = 1.025454, val_acc = 0.433127\n",
      "loss[b=5000] = 1.024723, val_acc = 0.435934\n",
      "Epoch: 7\n",
      "loss[b=0000] = 1.020699, val_acc = 0.437262\n",
      "loss[b=1000] = 1.039756, val_acc = 0.439785\n",
      "loss[b=2000] = 1.026702, val_acc = 0.440448\n",
      "loss[b=3000] = 1.035732, val_acc = 0.445570\n",
      "loss[b=4000] = 1.014438, val_acc = 0.444280\n",
      "loss[b=5000] = 1.015595, val_acc = 0.447448\n",
      "Epoch: 8\n",
      "loss[b=0000] = 1.008629, val_acc = 0.447960\n",
      "loss[b=1000] = 1.029365, val_acc = 0.449743\n",
      "loss[b=2000] = 1.013635, val_acc = 0.451811\n",
      "loss[b=3000] = 1.024642, val_acc = 0.454637\n",
      "loss[b=4000] = 1.002721, val_acc = 0.453821\n",
      "loss[b=5000] = 1.005340, val_acc = 0.456837\n",
      "Epoch: 9\n",
      "loss[b=0000] = 0.995846, val_acc = 0.459607\n",
      "loss[b=1000] = 1.018636, val_acc = 0.458374\n",
      "loss[b=2000] = 1.000652, val_acc = 0.461750\n",
      "loss[b=3000] = 1.013458, val_acc = 0.462945\n",
      "loss[b=4000] = 0.991058, val_acc = 0.463002\n",
      "loss[b=5000] = 0.995749, val_acc = 0.464671\n",
      "Optimization done\n",
      "Train accuracy: 0.5058473\n",
      "Test accuracy: 0.4615778\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# Start as session\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the initializer\n",
    "    session.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(10):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for b in range(0, X_train.shape[0], batch_size):\n",
    "            _, loss_value = session.run([optimizer, loss], feed_dict={X: X_train[b:b+batch_size],\n",
    "                                                                      y: y_train[b:b+batch_size],\n",
    "                                                                      learning_rate: 0.0001})\n",
    "            \n",
    "            if b % 1000 == 0:\n",
    "                validation_accuracy = session.run(accuracy, feed_dict={X: X_validation, y: y_validation})\n",
    "                print(\"loss[b=%04d] = %f, val_acc = %f\" % (b, loss_value, validation_accuracy))        \n",
    "        \n",
    "    print(\"Optimization done\")\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy_value, pred_train = session.run([accuracy, prediction], feed_dict={X: X_train, y: y_train})\n",
    "    print(\"Train accuracy:\", train_accuracy_value)\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy_value, pred_test = session.run([accuracy, prediction], feed_dict={X: X_test, y: y_test})\n",
    "    print(\"Test accuracy:\", test_accuracy_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
